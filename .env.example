# AI Policy Helper - Environment Configuration
# Copy this file to .env and update values as needed

# ============================================
# LLM PROVIDER CONFIGURATION
# ============================================
# Options: openai | ollama | stub
# - openai: Use OpenAI GPT-4o-mini (requires OPENAI_API_KEY)
# - ollama: Use local Ollama instance
# - stub: Use deterministic stub LLM (works offline)
LLM_PROVIDER=openai

# OpenAI API Key (required when LLM_PROVIDER=openai)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=ssk-proj-WxwK82E-jGjM7J6dc4Wh6qyvIzzIvOL3EhUhrHPPzwvRWPD9ONagj3LBLZgNE1bDTtT_b0RyMjT3BlbkFJ88fYzGw2XrG8yqjuQ06K1rLlCbHmkj-gjE5QvqirUVEZmUy-YETnMS-90a2nUB71LK3H79cxkA
# Ollama host (only used when LLM_PROVIDER=ollama)
OLLAMA_HOST=http://ollama:11434

# ============================================
# VECTOR STORE CONFIGURATION
# ============================================
# Options: qdrant | memory
# - qdrant: Use Qdrant vector database (recommended)
# - memory: Use in-memory store (for testing only)
VECTOR_STORE=qdrant

# Qdrant connection URL (when VECTOR_STORE=qdrant)
QDRANT_URL=http://qdrant:6333

# Qdrant collection name
COLLECTION_NAME=policy_helper

# ============================================
# EMBEDDING CONFIGURATION
# ============================================
# Embedding model identifier
# Options:
# - local-384: Hash-based embeddings (fast, deterministic, but poor semantic quality)
# - sentence-transformers/all-MiniLM-L6-v2: Semantic embeddings (recommended, ~80MB download)
# - sentence-transformers/all-mpnet-base-v2: Higher quality (larger model, ~420MB)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# ============================================
# DOCUMENT PROCESSING CONFIGURATION
# ============================================
# Chunk size in tokens (recommended: 500-1000)
# Larger chunks = more context but slower retrieval
CHUNK_SIZE=700

# Overlap between chunks in tokens (recommended: 50-100)
# Larger overlap = better context preservation but more storage
CHUNK_OVERLAP=80

# Data directory path (mounted in Docker)
DATA_DIR=/app/data

# ============================================
# FRONTEND CONFIGURATION
# ============================================
# Backend API base URL (for frontend API calls)
# Use http://localhost:8000 for local development
NEXT_PUBLIC_API_BASE=http://localhost:8000

# ============================================
# DEPLOYMENT NOTES
# ============================================
# For Docker Compose deployment:
# - Keep QDRANT_URL=http://qdrant:6333 (uses service name)
# - Keep NEXT_PUBLIC_API_BASE=http://localhost:8000
#
# For local development (without Docker):
# - Change QDRANT_URL=http://localhost:6333
# - Ensure Qdrant is running: docker run -p 6333:6333 qdrant/qdrant
#
# For production:
# - Use environment-specific API keys
# - Set proper CORS origins in backend/app/main.py
# - Consider using managed Qdrant Cloud
# - Enable HTTPS/TLS
