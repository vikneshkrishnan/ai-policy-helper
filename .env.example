# AI Policy Helper - Environment Configuration
# Copy this file to .env and update values as needed

# ============================================
# LLM PROVIDER CONFIGURATION
# ============================================
# Options: openai | ollama | stub
# - openai: Use OpenAI GPT-4o-mini (REQUIRED FOR DEMO - better answers, accurate citations)
# - stub: Use deterministic stub LLM (fast, works offline, but poor retrieval quality)
# - ollama: Use local Ollama instance
#
# NOTE: Demo/submission REQUIRES OpenAI (as per requirements).
# Stub is available for development/testing only.
LLM_PROVIDER=openai

# OpenAI API Key (REQUIRED when LLM_PROVIDER=openai)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here
# Ollama host (only used when LLM_PROVIDER=ollama)
OLLAMA_HOST=http://ollama:11434

# ============================================
# VECTOR STORE CONFIGURATION
# ============================================
# Options: qdrant | memory
# - qdrant: Use Qdrant vector database (recommended)
# - memory: Use in-memory store (for testing only)
VECTOR_STORE=qdrant

# Qdrant connection URL (when VECTOR_STORE=qdrant)
QDRANT_URL=http://qdrant:6333

# Qdrant collection name
COLLECTION_NAME=policy_helper

# ============================================
# EMBEDDING CONFIGURATION
# ============================================
# Embedding model identifier
# Options:
# - sentence-transformers/all-MiniLM-L6-v2: Semantic embeddings (RECOMMENDED - accurate retrieval)
# - sentence-transformers/all-mpnet-base-v2: Higher quality (larger model, ~420MB)
# - local-384: Hash-based embeddings (fast startup, but poor semantic quality - dev only)
#
# NOTE: Demo/submission should use sentence-transformers for correct document retrieval.
# Hash-based embeddings will retrieve wrong documents and fail acceptance tests.
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# ============================================
# DOCUMENT PROCESSING CONFIGURATION
# ============================================
# Chunk size in tokens (recommended: 500-1000)
# Larger chunks = more context but slower retrieval
CHUNK_SIZE=700

# Overlap between chunks in tokens (recommended: 50-100)
# Larger overlap = better context preservation but more storage
CHUNK_OVERLAP=80

# Data directory path (mounted in Docker)
DATA_DIR=/app/data

# ============================================
# FRONTEND CONFIGURATION
# ============================================
# Backend API base URL (for frontend API calls)
# Use http://localhost:8000 for local development
NEXT_PUBLIC_API_BASE=http://localhost:8000

# ============================================
# DEPLOYMENT NOTES
# ============================================
# For Docker Compose deployment:
# - Keep QDRANT_URL=http://qdrant:6333 (uses service name)
# - Keep NEXT_PUBLIC_API_BASE=http://localhost:8000
#
# For local development (without Docker):
# - Change QDRANT_URL=http://localhost:6333
# - Ensure Qdrant is running: docker run -p 6333:6333 qdrant/qdrant
#
# For production:
# - Use environment-specific API keys
# - Set proper CORS origins in backend/app/main.py
# - Consider using managed Qdrant Cloud
# - Enable HTTPS/TLS
